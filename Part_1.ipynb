{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma, loggamma\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dirichlet:\n",
    "    \n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    ## In the following functions:\n",
    "    \n",
    "    ## probs correspond to the prior predictive distribution probabilities\n",
    "    ## expert_probs correspond to the elicited probabilities from the expert\n",
    "    \n",
    "    ## sample_probs and sample_expert_probs are the same quantities but for multiple sets of covariates (J), each of which may have different partitions\n",
    "    \n",
    "    ## For both sample_probs and sample_expert_probs, the probabilities for each j = 1,...,J are in the j'th row\n",
    "    \n",
    "    \n",
    "    ## Function to calculate the approximation of the MLE of alpha for J=1\n",
    "        \n",
    "    def alpha_mle(self, probs, expert_probs):\n",
    "        \n",
    "        #assert probs.ndim == 1 and expert_probs.ndim == 1, \"This operation requires one set of probabilities only\"\n",
    "        assert np.isclose(np.sum(probs), 1) and np.isclose(np.sum(expert_probs), 1), \"Probabilities must sum to 1\"\n",
    "        \n",
    "        K = len(probs)\n",
    "        \n",
    "        kl_divergence = - np.sum([probs[k]*(np.log(expert_probs[k]) - np.log(probs[k])) for k in range(K)])\n",
    "        \n",
    "        return (K/2 - 1/2) / kl_divergence\n",
    "    \n",
    "    \n",
    "    ## Function to calculate the same quantity for J>1\n",
    "        \n",
    "    def alpha_mle_multiple_samples(self, sample_probs, sample_expert_probs):\n",
    "        \n",
    "        J = len(sample_probs) if type(sample_probs[0]) in [list, np.ndarray] else 1\n",
    "                \n",
    "        if J == 1: return self.alpha_mle(sample_probs, sample_expert_probs)\n",
    "                \n",
    "        assert np.all(np.isclose(np.array([np.sum(probs) for probs in sample_probs]), np.ones(J))) and np.all(np.isclose(np.array([np.sum(probs) for probs in sample_expert_probs]), np.ones(J))), \"Probabilities must sum to 1\"\n",
    "        \n",
    "        nom = 0\n",
    "        den = 0\n",
    "        \n",
    "        for j in range(J):\n",
    "            \n",
    "            n_j = len(sample_probs[j])\n",
    "            \n",
    "            nom += (n_j - 1)/2\n",
    "            \n",
    "            kl_divergence = - np.sum([sample_probs[j][k]*(np.log(sample_expert_probs[j][k]) - np.log(sample_probs[j][k])) for k in range(n_j)])\n",
    "            \n",
    "            den += kl_divergence\n",
    "            \n",
    "        return nom / den\n",
    "    \n",
    "    ## Simple function for the PDF of the Dirichet distribution\n",
    "    \n",
    "    def pdf(self, probs, expert_probs):\n",
    "        \n",
    "        #assert probs.ndim == 1 and expert_probs.ndim == 1, \"Pdf is defined for one set of probabilities only\"\n",
    "        assert np.isclose(np.sum(probs), 1) and np.isclose(np.sum(expert_probs), 1), \"Probabilities must sum to 1\"\n",
    "        \n",
    "        reset = 0\n",
    "        \n",
    "        if self.alpha is None:\n",
    "            reset = 1\n",
    "            self.alpha = self.alpha_mle_multiple_samples(probs, expert_probs)\n",
    "        \n",
    "        num_1 = gamma(self.alpha)\n",
    "        den_1 = np.prod([gamma(self.alpha*prob) for prob in probs])\n",
    "        pt_1 = num_1 / den_1\n",
    "                \n",
    "        pt_2 = np.prod([expert_probs[i]**(self.alpha*probs[i] - 1) for i in range(len(probs))])\n",
    "        \n",
    "        if reset == 1: self.alpha = None\n",
    "        \n",
    "        return pt_1 * pt_2\n",
    "    \n",
    "    \n",
    "    ## Function for log likelihood for J=1\n",
    "        \n",
    "    def llik(self, probs, expert_probs):\n",
    "                \n",
    "        #assert probs.ndim == 1 and expert_probs.ndim == 1, \"Likelihood is defined for one set of probabilities only\"\n",
    "        \n",
    "        assert np.isclose(np.sum(probs), 1) and np.isclose(np.sum(expert_probs), 1), \"Probabilities must sum to 1\"\n",
    "        \n",
    "        reset = 0\n",
    "        \n",
    "        if self.alpha is None:\n",
    "            reset = 1\n",
    "            self.alpha = self.alpha_mle_multiple_samples(probs, expert_probs)\n",
    "        \n",
    "        loggamma_alpha = loggamma(self.alpha)\n",
    "        \n",
    "        num_1 = loggamma_alpha\n",
    "        den_1 = np.sum([loggamma_alpha + loggamma(prob) for prob in probs])\n",
    "        pt_1 = num_1 - den_1\n",
    "        \n",
    "        pt_2 = np.sum([(self.alpha*probs[i] - 1) * np.log(expert_probs[i]) for i in range(len(probs))])\n",
    "        \n",
    "        if reset == 1: self.alpha = None\n",
    "        \n",
    "        return pt_1 + pt_2\n",
    "    \n",
    "    ## Sum of log-likelihoods. This will be used in later stages during optimization\n",
    "    \n",
    "    def sum_llik(self, sample_probs: list, sample_expert_probs: list):\n",
    "        \n",
    "        J = len(sample_probs) if type(sample_probs[0]) in [list, np.ndarray] else 1\n",
    "                \n",
    "        if J == 1: return self.llik(sample_probs, sample_expert_probs)\n",
    "        \n",
    "        assert np.all(np.isclose(np.array([np.sum(probs) for probs in sample_probs]), np.ones(J))) and np.all(np.isclose(np.array([np.sum(probs) for probs in sample_expert_probs]), np.ones(J))), \"Probabilities must sum to 1\"\n",
    "        \n",
    "        reset = 0\n",
    "        \n",
    "        if self.alpha is None:\n",
    "            reset = 1\n",
    "            self.alpha = self.alpha_mle_multiple_samples(sample_probs, sample_expert_probs)\n",
    "        \n",
    "        total_llik = 0\n",
    "        \n",
    "        for j in range(J):\n",
    "            \n",
    "            total_llik += self.llik(sample_probs[j], sample_expert_probs[j])\n",
    "            \n",
    "        if reset == 1: self.alpha = None\n",
    "            \n",
    "        return total_llik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class PPEProbabilities:\n",
    "    \n",
    "    def __init__(self, target_type, path):\n",
    "        self.target_type = target_type\n",
    "        self.path = path\n",
    "        \n",
    "    def get_expert_data(self, expert_input):        \n",
    "        \n",
    "        if self.target_type == \"discrete\":\n",
    "            \n",
    "            if self.path:\n",
    "                if os.path.isfile(expert_input):  ## Checking if the input is a single file or a folder (which is assumed to contain files). Note that if we have different number of partitions for different covariate sets, we need a folder to store them\n",
    "                    expert_input = pd.read_csv(expert_input, index_col=0)\n",
    "                    elicited_data = expert_input.to_numpy()\n",
    "                     \n",
    "                else: ## if not, then the path must lead to a folder containing multiple files. In the discrete case, we assume that each file contains the classes in column 1 and the probabilities at column 2\n",
    "                    \n",
    "                    files = os.listdir(expert_input)\n",
    "\n",
    "                    # Filter only CSV files\n",
    "                    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "                    \n",
    "                    elicited_covariate_sets = []\n",
    "\n",
    "                    # Loop through each CSV file and process its contents\n",
    "                    for csv_file in csv_files:\n",
    "                        df = pd.read_csv(expert_input + \"/\" + csv_file, index_col=0)\n",
    "                        elicited_covariate_set = df.to_numpy()\n",
    "                        \n",
    "                        elicited_covariate_sets.append(elicited_covariate_set)\n",
    "                        \n",
    "                    elicited_data = np.zeros((elicited_covariate_sets[0].shape[0], len(elicited_covariate_sets) + 1))\n",
    "                    \n",
    "                    elicited_data[:,0] = elicited_covariate_sets[0][:,0]\n",
    "                    \n",
    "                    for j, set in enumerate(elicited_covariate_sets):\n",
    "                        \n",
    "                        elicited_data[:,j+1] = set[:,-1]\n",
    "                        \n",
    "            else:\n",
    "                elicited_data = expert_input   ## the input is a matrix containing the classes and the corresponding probabilities\n",
    "                \n",
    "        \n",
    "            elicited_data = elicited_data.astype(float)  ## ensuring that all values are numerical      \n",
    "            \n",
    "            partitions = elicited_data[:,0]\n",
    "            \n",
    "            expert_probabilities = [elicited_data[:,j+1] for j in range(elicited_data.shape[1] - 1)]\n",
    "        \n",
    "        if self.target_type == \"continuous\":\n",
    "            \n",
    "            ## Goal format: J separate matrices that have three columns; the first two being the partitions and third being the corresponding probabilities\n",
    "            \n",
    "            if self.path:\n",
    "                if os.path.isfile(expert_input):  ## Checking if the input is a single file or a folder (which is assumed to contain files)\n",
    "                    expert_input = pd.read_csv(expert_input, index_col=0)\n",
    "                    elicited_data = expert_input.to_numpy()\n",
    "                     \n",
    "                else: ## if not, then the path must lead to a folder containing multiple files. In the continuous case, we assume that each file contains three columns; the first two being the partitions and third being the corresponding probabilities\n",
    "                    \n",
    "                    files = os.listdir(expert_input)\n",
    "\n",
    "                    # Filter only CSV files\n",
    "                    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "                    \n",
    "                    elicited_data = []\n",
    "\n",
    "                    # Loop through each CSV file and process its contents\n",
    "                    for csv_file in csv_files:\n",
    "                        df = pd.read_csv(expert_input + \"/\" + csv_file, index_col=0)\n",
    "                        elicited_covariate_set = df.to_numpy()\n",
    "                        \n",
    "                        elicited_data.append(elicited_covariate_set)\n",
    "                        \n",
    "                    \n",
    "                        \n",
    "            else:\n",
    "                elicited_data = expert_input   ## the input is a matrix containing the partitions and the corresponding probabilities\n",
    "                \n",
    "        \n",
    "            elicited_data = [cov_set.astype(float) for cov_set in elicited_data]  ## ensuring that all values are numerical\n",
    "            \n",
    "            partitions = [covariate_set[:,[0,1]] for covariate_set in elicited_data]\n",
    "            expert_probabilities = [covariate_set[:, -1] for covariate_set in elicited_data]\n",
    "            \n",
    "            \n",
    "        return partitions, expert_probabilities\n",
    "    \n",
    "    ## discrete data: \"partitions\" are an array containing the classes and \"expert_probabilities\" are a matrix with one column for each J\n",
    "    ## continuous data: \"partitions\" are a list of length J, containing one partition for each covariate set and \"expert_probabilities\" is a list of same length, containing the respective probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "    def ppd_probs(self, samples, partitions):\n",
    "                \n",
    "        \n",
    "        if self.target_type == \"discrete\":\n",
    "                \n",
    "            J = samples.shape[1] ## Each column in \"samples\" corresponds to one set of covariates\n",
    "            \n",
    "            N_samples = samples.shape[0]\n",
    "            \n",
    "            N_classes = len(partitions)\n",
    "            \n",
    "            ## Here, the samples come from the prior predictive distribution and contain values for y, which is discrete\n",
    "            \n",
    "            ## In order to get the probabilities for each class c, we simply compute #(sample = c) / #(sample)\n",
    "            \n",
    "            model_probabilities = []\n",
    "            \n",
    "            \n",
    "            for j in range(J):\n",
    "                \n",
    "                cov_set_j = samples[:,j]\n",
    "                \n",
    "                probs_list = np.zeros(N_classes)\n",
    "                \n",
    "                for i,C in enumerate(partitions):\n",
    "                                        \n",
    "                    probs_list[i] = np.sum(cov_set_j == C) / N_samples\n",
    "                                    \n",
    "                model_probabilities.append(probs_list)\n",
    "                \n",
    "                \n",
    "        if self.target_type == \"continuous\":\n",
    "                        \n",
    "            \n",
    "            J = samples.shape[1] ## Each column in \"samples\" corresponds to one set of covariates\n",
    "    \n",
    "            N_samples = samples.shape[0]\n",
    "                \n",
    "            ## We want the same format as the one of the elicited probabilities. For that reason,\n",
    "            ## the output will be a list of probabilities\n",
    "            \n",
    "            model_probabilities = []\n",
    "            \n",
    "            for j in range(J):\n",
    "                \n",
    "                \n",
    "                partition = np.copy(partitions[j])\n",
    "                cov_set_j = samples[:,j]\n",
    "                \n",
    "                N_partitions = partition.shape[0]\n",
    "                \n",
    "                ## When sampling, it is possible that we get a value that is outside the partitions. In that case, we redifine the bounds according to the sampled value\n",
    "                ## E.g. if the lower bound among all partitions is 15 and we sample the value 12, the new lower bound will be 12\n",
    "                ## This however should not happen too often in the sampling process, as the lower and upper bounds should be wide enough to contain all samples\n",
    "\n",
    "                sample_min = np.min(cov_set_j)\n",
    "                sample_max = np.max(cov_set_j)\n",
    "                \n",
    "                if partition[0,0] > sample_min:\n",
    "                    partition[0,0] = sample_min\n",
    "                    \n",
    "                if partition[-1,1] < sample_max:\n",
    "                    partition[-1,1] = sample_max\n",
    "                    \n",
    "                probs_list = np.zeros(N_partitions)\n",
    "                    \n",
    "                for i in range(N_partitions):\n",
    "                    \n",
    "                    lower_bound = partition[i][0]\n",
    "                    upper_bound = partition[i][1]\n",
    "                                        \n",
    "                    count = np.sum((cov_set_j >= lower_bound) & (cov_set_j <= upper_bound))\n",
    "                    \n",
    "                    probs_list[i] = count / N_samples\n",
    "                                            \n",
    "                model_probabilities.append(probs_list)\n",
    "\n",
    "\n",
    "        return model_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple checks regarding the Dirichlet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17835805432006951\n",
      "-1.7239622080059016\n",
      "19.978004541555926\n",
      "True\n",
      "-----------------\n",
      "-2.7055326287365427\n",
      "24.515109566030482\n",
      "----------------\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "### Some simple checks:\n",
    "\n",
    "\n",
    "dir_1 = Dirichlet(1)\n",
    "\n",
    "incorrect_probs = np.array([0.5,0.1,0.2,0.3])\n",
    "expert_probs = np.array([0.2,0.15,0.25,0.4])\n",
    "\n",
    "## both give an error, as they should\n",
    "\n",
    "## print(dir_1.alpha_mle(incorrect_probs, expert_probs))\n",
    "## print(dir_1.alpha_mle_multiple_samples(incorrect_probs, expert_probs))\n",
    "\n",
    "        \n",
    "probs = np.array([0.3,0.05,0.2,0.45])\n",
    "expert_probs = np.array([0.2,0.15,0.25,0.4])\n",
    "\n",
    "\n",
    "print(dir_1.pdf(probs, expert_probs))\n",
    "print(dir_1.llik(probs, expert_probs))\n",
    "print(dir_1.alpha_mle(probs, expert_probs))\n",
    "\n",
    "\n",
    "dir_2 = Dirichlet(None) \n",
    "dir_3 = Dirichlet(dir_1.alpha_mle(probs, expert_probs))\n",
    "\n",
    "##checking that if alpha is None, then it is computed based on dirichlet mle\n",
    "\n",
    "print(dir_2.llik(probs, expert_probs) == dir_3.llik(probs, expert_probs)) \n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "sample_incorrect_probs = [[0.3,0.05,0.2,0.45], [0.35,0.05,0.2,0.45]]\n",
    "sample_expert_probs = [[0.2,0.15,0.25,0.4], [0.2,0.15,0.25,0.4]]\n",
    "\n",
    "## Gives an error\n",
    "\n",
    "## print(dir_1.sum_llik(sample_incorrect_probs, sample_expert_probs))\n",
    "\n",
    "\n",
    "\n",
    "sample_probs = [np.array([0.3,0.05,0.2,0.45]), np.array([0.15,0.25,0.5,0.1])]\n",
    "sample_expert_probs = [np.array([0.2,0.15,0.25,0.4]), np.array([0.1,0.2,0.5,0.2])]\n",
    "\n",
    "print(dir_1.sum_llik(sample_probs, sample_expert_probs))\n",
    "print(dir_1.alpha_mle_multiple_samples(sample_probs, sample_expert_probs))\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "\n",
    "dir_4 = Dirichlet(dir_1.alpha_mle_multiple_samples(sample_probs, sample_expert_probs))\n",
    "\n",
    "##checking that if alpha is None, then it is computed based on dirichlet mle\n",
    "\n",
    "print(dir_4.sum_llik(sample_probs, sample_expert_probs) == dir_2.sum_llik(sample_probs, sample_expert_probs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.237703436114796\n",
      "4.057439841963832\n"
     ]
    }
   ],
   "source": [
    "## Checking that having different probability dimensions is ok (necessary for having different number of partitions)\n",
    "\n",
    "dir_1 = Dirichlet(None) \n",
    "\n",
    "\n",
    "sample_probs = [np.array([0.3,0.05,0.2,0.45]), np.array([0.3, 0.25, 0.45])] ## 4 and 3 probabilities (partitions)\n",
    "sample_expert_probs = [np.array([0.2,0.15,0.25,0.4]), np.array([0.2, 0.7, 0.1])]\n",
    "\n",
    "\n",
    "\n",
    "print(dir_1.sum_llik(sample_probs, sample_expert_probs))\n",
    "print(dir_1.alpha_mle_multiple_samples(sample_probs, sample_expert_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running some tests to ensure that the two classes work as expected\n",
    "\n",
    "## Discrete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[array([0.3, 0.1, 0.6]), array([0.4, 0.4, 0.2]), array([0.5, 0.3, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############### Getting the data from a folder ###############\n",
    "\n",
    "probs_1 = np.array([[0, 1, 2], [0.5, 0.3, 0.2]]).T\n",
    "probs_2 = np.array([[0, 1, 2], [0.4, 0.4, 0.2]]).T\n",
    "probs_3 = np.array([[0, 1, 2], [0.3, 0.1, 0.6]]).T\n",
    "\n",
    "# Folder path to store CSV files\n",
    "folder_path = '/Users/panos/Desktop/Internship/test_folder'\n",
    "\n",
    "probs_1 = pd.DataFrame(probs_1)\n",
    "probs_2 = pd.DataFrame(probs_2)\n",
    "probs_3 = pd.DataFrame(probs_3)\n",
    "\n",
    "probs_1.to_csv(folder_path + \"/probs_1.csv\")\n",
    "probs_2.to_csv(folder_path + \"/probs_2.csv\")\n",
    "probs_3.to_csv(folder_path + \"/probs_3.csv\")\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=True)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(folder_path)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.6])]\n"
     ]
    }
   ],
   "source": [
    "############### Getting the data from a file ###############\n",
    "\n",
    "probs_file = np.array([[0, 1, 2], [0.5, 0.3, 0.2], [0.4, 0.4, 0.2], [0.3, 0.1, 0.6]]).T\n",
    "\n",
    "# Path to store the CSV file\n",
    "\n",
    "probs_file = pd.DataFrame(probs_file)\n",
    "\n",
    "probs_file.to_csv('/Users/panos/Desktop/Internship/probs_file.csv')\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=True)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data('/Users/panos/Desktop/Internship/probs_file.csv')\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.6])]\n"
     ]
    }
   ],
   "source": [
    "############### Feeding the data directly ###############\n",
    " \n",
    "\n",
    "probs = np.array([[0, 1, 2], [0.5, 0.3, 0.2], [0.4, 0.4, 0.2], [0.3, 0.1, 0.6]]).T\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[array([0.3, 0.7]), array([0.6, 0.4])]\n",
      "[array([0.2915, 0.7085]), array([0.599, 0.401])]\n",
      "------------\n",
      "5712.568749470156\n",
      "-94695.3210428995\n",
      "-37.9571996999857\n"
     ]
    }
   ],
   "source": [
    "######## Test where we input samples from a simple prior predictive distribution and get model probabilities for the partitions ########\n",
    "\n",
    "elicited_data_discrete = np.array([[0,1],[0.3,0.7],[0.6,0.4]]).T\n",
    "\n",
    "\n",
    "cov_set_1 = np.random.binomial(n = 1, p = 0.7, size = 2000)\n",
    "cov_set_2 = np.random.binomial(n = 1, p = 0.4, size = 2000)\n",
    "\n",
    "samples = np.vstack((cov_set_1, cov_set_2)).T\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"discrete\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(elicited_data_discrete)\n",
    "\n",
    "model_probs = prob_class.ppd_probs(samples, partitions)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)\n",
    "print(model_probs)\n",
    "print(\"------------\")\n",
    "\n",
    "## Feeding these probabilities to dirichlet\n",
    "\n",
    "dir = Dirichlet(None)\n",
    "\n",
    "print(dir.alpha_mle_multiple_samples(model_probs, expert_probs))  ## very high alpha, which makes sense since we used the \"expert\" probabilities to sample\n",
    "\n",
    "print(dir.sum_llik(model_probs, expert_probs))\n",
    "\n",
    "dir_2 = Dirichlet(10) ## trying fixed alpha\n",
    "\n",
    "print(dir_2.sum_llik(model_probs, expert_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. 100.]\n",
      " [100. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 100.]\n",
      " [100. 200.]\n",
      " [200. 300.]]\n",
      "[array([0.3, 0.1, 0.4, 0.2]), array([0.4, 0.4, 0.2]), array([0.5, 0.3, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############### Getting the data from a folder ###############\n",
    "\n",
    "probs_1 = np.array([[0, 100, 200], [100, 200, 300], [0.5, 0.3, 0.2]]).T  ## partitions (0,100), (100, 200), (200, 300)\n",
    "probs_2 = np.array([[0, 150, 200], [150, 200, 300], [0.4, 0.4, 0.2]]).T  ## partitions (0,150), (150, 200), (200, 300)\n",
    "probs_3 = np.array([[0, 100, 150, 200], [100, 150, 200, 300], [0.3, 0.1, 0.4, 0.2]]).T   ## Different number of partitions here! partitions (0,100), (100, 150), (150, 200), (200, 300)\n",
    "\n",
    "# Folder path to store CSV files\n",
    "folder_path = '/Users/panos/Desktop/Internship/test_folder_2'\n",
    "\n",
    "probs_1 = pd.DataFrame(probs_1)\n",
    "probs_2 = pd.DataFrame(probs_2)\n",
    "probs_3 = pd.DataFrame(probs_3)\n",
    "\n",
    "probs_1.to_csv(folder_path + \"/probs_1.csv\")\n",
    "probs_2.to_csv(folder_path + \"/probs_2.csv\")\n",
    "probs_3.to_csv(folder_path + \"/probs_3.csv\")\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=True)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(folder_path)\n",
    "\n",
    "for partition in partitions:\n",
    "    print(partition)\n",
    "\n",
    "print(expert_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. 100.]\n",
      " [100. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[[  0. 100.]\n",
      " [100. 150.]\n",
      " [150. 200.]\n",
      " [200. 300.]]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.4, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "############### Feeding the data directly ###############\n",
    "\n",
    "probs_1 = np.array([[0, 100, 200], [100, 200, 300], [0.5, 0.3, 0.2]]).T\n",
    "probs_2 = np.array([[0, 150, 200], [150, 200, 300], [0.4, 0.4, 0.2]]).T\n",
    "probs_3 = np.array([[0, 100, 150, 200], [100, 150, 200, 300], [0.3, 0.1, 0.4, 0.2]]).T   ## Different number of partitions here!\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3]\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "for partition in partitions:\n",
    "    print(partition)\n",
    "\n",
    "print(expert_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  0., 100.],\n",
      "       [100., 200.],\n",
      "       [200., 300.]]), array([[  0., 150.],\n",
      "       [150., 200.],\n",
      "       [200., 300.]]), array([[  0., 100.],\n",
      "       [100., 150.],\n",
      "       [150., 200.],\n",
      "       [200., 300.]])]\n",
      "[array([0.5, 0.3, 0.2]), array([0.4, 0.4, 0.2]), array([0.3, 0.1, 0.4, 0.2])]\n",
      "[array([0.7788, 0.22  , 0.0012]), array([0.5072, 0.415 , 0.0778]), array([0.0024, 0.0749, 0.4232, 0.4995])]\n",
      "------------\n",
      "4.482511758834674\n",
      "-40.166197918365945\n",
      "-130.1189527892806\n"
     ]
    }
   ],
   "source": [
    "######## Test where we input samples from a simple prior predictive distribution and get model probabilities for the partitions ########\n",
    "\n",
    "probs_1 = np.array([[0, 100, 200], [100, 200, 300], [0.5, 0.3, 0.2]]).T\n",
    "probs_2 = np.array([[0, 150, 200], [150, 200, 300], [0.4, 0.4, 0.2]]).T\n",
    "probs_3 = np.array([[0, 100, 150, 200], [100, 150, 200, 300], [0.3, 0.1, 0.4, 0.2]]).T   ## Different number of partitions here!\n",
    "\n",
    "probs = [probs_1, probs_2, probs_3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "samples_1 = np.random.normal(loc = 66, scale = 45, size = 10000)\n",
    "samples_2 = np.random.normal(loc = 150, scale = 35, size = 10000)\n",
    "samples_3 = np.random.normal(loc = 200, scale = 35, size = 10000)\n",
    "\n",
    "samples = np.vstack((samples_1, samples_2, samples_3)).T\n",
    "\n",
    "\n",
    "prob_class = PPEProbabilities(target_type=\"continuous\", path=False)\n",
    "\n",
    "\n",
    "partitions, expert_probs = prob_class.get_expert_data(probs)\n",
    "\n",
    "model_probs = prob_class.ppd_probs(samples, partitions)\n",
    "\n",
    "print(partitions)\n",
    "print(expert_probs)\n",
    "print(model_probs)\n",
    "print(\"------------\")\n",
    "\n",
    "\n",
    "\n",
    "dir = Dirichlet(None)\n",
    "\n",
    "print(dir.alpha_mle_multiple_samples(model_probs, expert_probs))  ## low alpha, which makes sense since we used a \"random\" distribution to sample, loosely following the expert distribution\n",
    "\n",
    "print(dir.sum_llik(model_probs, expert_probs))\n",
    "\n",
    "dir_2 = Dirichlet(10) ## trying fixed alpha\n",
    "\n",
    "print(dir_2.sum_llik(model_probs, expert_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppe2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
