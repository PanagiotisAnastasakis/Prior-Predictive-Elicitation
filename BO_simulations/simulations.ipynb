{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations of PPE using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will perform simulations for known probabilistic distributions of a target $Y$, aiming to evaluate the effectiveness of Bayesian Optimization (BO) for inferring an optimal set of hyperparameters. To achieve that, we will assume a probabilistic distribution $Y | \\pmb{\\theta} \\sim \\pi_{Y | \\pmb{\\theta}}$, where $\\pmb{\\theta}\\sim\\pi_{\\pmb{\\theta}}$, with $\\pi_{\\pmb{\\theta}}$ belonging to a family of distributions that is indexed by a hyperparameter vector $\\pmb{\\lambda}$. For a fixed value $\\pmb{\\lambda_{\\text{true}}}$, we will obtain simulated expert probabilities for a given partition, which we will subsequently use to perform PPE using BO. The experiment will involve partitions with different number of bins, and also different number of covariates $J$, whenever the target is dependent on a set of covariates, to test how efficient the method is with more detailed partitions and also more covariate sets for which there is expert input.\n",
    "\n",
    "We will run simulations for three different families of probabilistic models. The first is the gaussian family, where we assume that $$Y\\sim \\mathcal{N}(\\mu, \\sigma^2),$$ with different possible priors then used to define $\\mu$ and $\\sigma$.\n",
    "\n",
    "The second probabilistic model family again assumes that $Y$ is drawn from a gaussian distribution, but now the mean is estimated as a linear combination of a set of covariates $x_1,...,x_n$:\n",
    "$$Y\\sim\\mathcal{N}(b_0 + \\prod_{i=1}^{n}b_i\\cdot x_i, \\sigma^2),$$\n",
    "\n",
    "where $b_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i)$.\n",
    "\n",
    "The third family is that of logistic regression, where $Y$ is now binary and a function of covariates x = \\{$x_1,...,x_n$\\}. In probabilistic notation, we have:\n",
    "\n",
    "$$Y\\sim\\mathcal{B}(p(\\pmb{x}, \\pmb{\\theta})),$$\n",
    "\n",
    "where $p(\\pmb{x}, \\pmb{\\theta}) = \\frac{e^{\\pmb{x}^{\\Tau} \\pmb{\\theta}}}{1 + e^{\\pmb{x}^{\\Tau} \\pmb{\\theta}}}$, with $\\theta_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import make_partition, ppe_simulation, make_plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Family 1: Gaussian distribution with no covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model family, we will test two models, namely Model 1 and Model 2. For each, we will implement PPE with BO, considering multiple partitionings, each with a different number of bins. We will test partitionings with 2, 5, 10 and 20 bins. The way we construct the partitionings is by taking an area where $Y$ is \"more likely to be into\", and partition it according to the number of bins. Then, we also consider two additional partitions for the lower and upper tails of the distribution so that the total number of bins is the one desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_models import gaussian_model_1, gaussian_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.e+04,  5.e+00],\n",
       "        [ 5.e+00,  1.e+04]]),\n",
       " array([[-1.00000000e+04, -1.50000000e+01],\n",
       "        [-1.50000000e+01, -1.66666667e+00],\n",
       "        [-1.66666667e+00,  1.16666667e+01],\n",
       "        [ 1.16666667e+01,  2.50000000e+01],\n",
       "        [ 2.50000000e+01,  1.00000000e+04]]),\n",
       " array([[-1.0e+04, -1.5e+01],\n",
       "        [-1.5e+01, -1.0e+01],\n",
       "        [-1.0e+01, -5.0e+00],\n",
       "        [-5.0e+00,  0.0e+00],\n",
       "        [ 0.0e+00,  5.0e+00],\n",
       "        [ 5.0e+00,  1.0e+01],\n",
       "        [ 1.0e+01,  1.5e+01],\n",
       "        [ 1.5e+01,  2.0e+01],\n",
       "        [ 2.0e+01,  2.5e+01],\n",
       "        [ 2.5e+01,  1.0e+04]]),\n",
       " array([[-1.00000000e+04, -1.50000000e+01],\n",
       "        [-1.50000000e+01, -1.27777778e+01],\n",
       "        [-1.27777778e+01, -1.05555556e+01],\n",
       "        [-1.05555556e+01, -8.33333333e+00],\n",
       "        [-8.33333333e+00, -6.11111111e+00],\n",
       "        [-6.11111111e+00, -3.88888889e+00],\n",
       "        [-3.88888889e+00, -1.66666667e+00],\n",
       "        [-1.66666667e+00,  5.55555556e-01],\n",
       "        [ 5.55555556e-01,  2.77777778e+00],\n",
       "        [ 2.77777778e+00,  5.00000000e+00],\n",
       "        [ 5.00000000e+00,  7.22222222e+00],\n",
       "        [ 7.22222222e+00,  9.44444444e+00],\n",
       "        [ 9.44444444e+00,  1.16666667e+01],\n",
       "        [ 1.16666667e+01,  1.38888889e+01],\n",
       "        [ 1.38888889e+01,  1.61111111e+01],\n",
       "        [ 1.61111111e+01,  1.83333333e+01],\n",
       "        [ 1.83333333e+01,  2.05555556e+01],\n",
       "        [ 2.05555556e+01,  2.27777778e+01],\n",
       "        [ 2.27777778e+01,  2.50000000e+01],\n",
       "        [ 2.50000000e+01,  1.00000000e+04]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_bins = np.array([2, 5, 10, 20])\n",
    "\n",
    "gaussian_partitions = [make_partition(n, 5 - 20, 5 + 20) for n in num_bins]\n",
    "\n",
    "gaussian_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "\n",
    "For the first model, we assume that $Y\\sim \\mathcal{N}(\\mu, \\sigma^2)$, where $\\mu \\sim \\mathcal{N}(\\mu_1, \\sigma_1)$ and $\\sigma \\sim \\text{Gamma}(a, b)$. Our hyperparameter vector is then $\\pmb{\\lambda} = [\\mu_1, \\sigma_1, a, b]$. For the simulation, we assume that $\\mu_1 = 5, \\sigma_1 = 2, a = 2$ and $b = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [Y_obs, mu, sigma]\n",
      "[INFO 07-01 17:10:28] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter mu_1. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 07-01 17:10:28] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter sigma_1. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 07-01 17:10:28] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter a. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 07-01 17:10:28] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter b. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 07-01 17:10:28] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter alpha. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 07-01 17:10:28] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='mu_1', parameter_type=FLOAT, range=[-10.0, 10.0]), RangeParameter(name='sigma_1', parameter_type=FLOAT, range=[0.001, 10.0]), RangeParameter(name='a', parameter_type=FLOAT, range=[0.001, 10.0]), RangeParameter(name='b', parameter_type=FLOAT, range=[0.001, 10.0]), RangeParameter(name='alpha', parameter_type=FLOAT, range=[0.001, 70.0])], parameter_constraints=[]).\n",
      "[INFO 07-01 17:10:28] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 07-01 17:10:28] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=5 num_trials=None use_batch_trials=False\n",
      "[INFO 07-01 17:10:28] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=10\n",
      "[INFO 07-01 17:10:28] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=10\n",
      "[INFO 07-01 17:10:28] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 07-01 17:10:28] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 10 trials, BoTorch for subsequent trials]). Iterations after 10 will take longer to generate due to model-fitting.\n",
      "[INFO 07-01 17:10:28] ax.service.managed_loop: Started full optimization with 75 steps.\n",
      "[INFO 07-01 17:10:28] ax.service.managed_loop: Running optimization trial 1...\n",
      "Sampling: [Y_obs, mu, sigma]\n",
      "[INFO 07-01 17:10:28] ax.service.managed_loop: Running optimization trial 2...\n",
      "Sampling: [Y_obs, mu, sigma]\n",
      "[ERROR 07-01 17:10:28] ax.service.managed_loop: Encountered exception during optimization: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/compile/function/types.py\", line 970, in __call__\n",
      "    self.vm()\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/graph/op.py\", line 515, in rval\n",
      "    r = p(n, [x[0] for x in i], o)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/tensor/random/op.py\", line 330, in perform\n",
      "    smpl_val = self.rng_fn(rng, *([*args, size]))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/tensor/random/op.py\", line 127, in rng_fn\n",
      "    return getattr(rng, self.name)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"numpy/random/_generator.pyx\", line 1220, in numpy.random._generator.Generator.normal\n",
      "  File \"_common.pyx\", line 616, in numpy.random._common.cont\n",
      "  File \"_common.pyx\", line 422, in numpy.random._common.check_constraint\n",
      "ValueError: scale < 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/ax/service/managed_loop.py\", line 237, in full_run\n",
      "    self.run_trial()\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/ax/utils/common/executils.py\", line 163, in actual_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/ax/service/managed_loop.py\", line 214, in run_trial\n",
      "    raw_data={\n",
      "             ^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/ax/service/managed_loop.py\", line 215, in <dictcomp>\n",
      "    arm.name: self._call_evaluation_function(arm.parameters, weight)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/ax/service/managed_loop.py\", line 153, in _call_evaluation_function\n",
      "    evaluation = self.evaluation_function(parameterization)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/Documents/GitHub/Prior-Predictive-Elicitation/ppe/bayesian_optimization.py\", line 171, in <lambda>\n",
      "    dir_neg_llik = lambda lam: self.dirichlet_neg_llik(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/Documents/GitHub/Prior-Predictive-Elicitation/ppe/bayesian_optimization.py\", line 70, in dirichlet_neg_llik\n",
      "    model_probs = self.get_model_probs(lam, partitions)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/Documents/GitHub/Prior-Predictive-Elicitation/ppe/bayesian_optimization.py\", line 41, in get_model_probs\n",
      "    idata = self.pymc_sampling_func(lam, self.target_samples) if num_samples is None else self.pymc_sampling_func(lam, num_samples)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/Documents/GitHub/Prior-Predictive-Elicitation/BO_simulations/gaussian_models.py\", line 17, in gaussian_model_1\n",
      "    idata = pm.sample_prior_predictive(random_seed=random_seed, samples = n_samples)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pymc/sampling/forward.py\", line 417, in sample_prior_predictive\n",
      "    values = zip(*(sampler_fn() for i in range(samples)))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pymc/sampling/forward.py\", line 417, in <genexpr>\n",
      "    values = zip(*(sampler_fn() for i in range(samples)))\n",
      "                   ^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/compile/function/types.py\", line 983, in __call__\n",
      "    raise_with_op(\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/link/utils.py\", line 523, in raise_with_op\n",
      "    raise exc_value.with_traceback(exc_trace)\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/compile/function/types.py\", line 970, in __call__\n",
      "    self.vm()\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/graph/op.py\", line 515, in rval\n",
      "    r = p(n, [x[0] for x in i], o)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/tensor/random/op.py\", line 330, in perform\n",
      "    smpl_val = self.rng_fn(rng, *([*args, size]))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/pytensor/tensor/random/op.py\", line 127, in rng_fn\n",
      "    return getattr(rng, self.name)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"numpy/random/_generator.pyx\", line 1220, in numpy.random._generator.Generator.normal\n",
      "  File \"_common.pyx\", line 616, in numpy.random._common.cont\n",
      "  File \"_common.pyx\", line 422, in numpy.random._common.check_constraint\n",
      "ValueError: scale < 0\n",
      "Apply node that caused the error: normal_rv{0, (0, 0), floatX, True}(RandomGeneratorSharedVariable(<Generator(PCG64) at 0x3128983C0>), [], 11, -1.3057474233210087, -1.3057474233210087)\n",
      "Toposort index: 1\n",
      "Inputs types: [RandomGeneratorType, TensorType(int64, shape=(0,)), TensorType(int64, shape=()), TensorType(float64, shape=()), TensorType(float64, shape=())]\n",
      "Inputs shapes: ['No shapes', (0,), (), (), ()]\n",
      "Inputs strides: ['No strides', (0,), (), (), ()]\n",
      "Inputs values: [Generator(PCG64) at 0x3128983C0, array([], dtype=int64), array(11), array(-1.30574742), array(-1.30574742)]\n",
      "Outputs clients: [['output'], ['output', normal_rv{0, (0, 0), floatX, True}(RandomGeneratorSharedVariable(<Generator(PCG64) at 0x312898580>), [], 11, mu, sigma)]]\n",
      "\n",
      "HINT: Re-running with most PyTensor optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the PyTensor flag 'optimizer=fast_compile'. If that does not work, PyTensor optimizations can be disabled with 'optimizer=None'.\n",
      "HINT: Use the PyTensor flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.\n",
      "/Users/panos/anaconda3/envs/ppe3/lib/python3.11/site-packages/ax/core/data.py:286: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return cls(df=pd.concat(dfs, axis=0, sort=True))\n",
      "Sampling: [Y_obs, mu, sigma]\n",
      "Sampling: [Y_obs, mu, sigma]\n",
      "Sampling: [Y_obs, mu, sigma]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'param_names' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m alphas_gaussian_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(num_bins))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_bins\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 25\u001b[0m     expert_probs, best_params, best_probs, alpha \u001b[38;5;241m=\u001b[39m \u001b[43mppe_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgaussian_model_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mJ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mJ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mlambd_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambd_names_gaussian_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mlambd_true_vals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambd_true_vals_gaussian_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mlower_inner\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlower_inner_gaussian_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mupper_inner\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mupper_inner_gaussian_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mparam_bounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparam_bounds_gaussian_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mtarget_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     simulated_expert_probs_gaussian_1 \u001b[38;5;241m=\u001b[39m expert_probs\n\u001b[1;32m     38\u001b[0m     best_params_gaussian_1 \u001b[38;5;241m=\u001b[39m best_params\n",
      "File \u001b[0;32m~/Documents/GitHub/Prior-Predictive-Elicitation/BO_simulations/functions.py:104\u001b[0m, in \u001b[0;36mppe_simulation\u001b[0;34m(model, J, target_type, lambd_names, lambd_true_vals, alpha, num_bins, lower_inner, upper_inner, param_bounds, target_samples, n_trials)\u001b[0m\n\u001b[1;32m    101\u001b[0m     param_names \u001b[38;5;241m=\u001b[39m lambd_names \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m## We optimize alpha also\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     param_bounds \u001b[38;5;241m=\u001b[39m param_bounds \u001b[38;5;241m+\u001b[39m [[\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m70.\u001b[39m]]\n\u001b[0;32m--> 104\u001b[0m param_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mparam_names\u001b[49m)\n\u001b[1;32m    105\u001b[0m param_expected_vals \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(param_names) \u001b[38;5;66;03m## we only focus on the dirichlet log likelihood\u001b[39;00m\n\u001b[1;32m    107\u001b[0m best_params \u001b[38;5;241m=\u001b[39m BO\u001b[38;5;241m.\u001b[39moptimize_hyperparams(param_names\u001b[38;5;241m=\u001b[39mparam_names,\n\u001b[1;32m    108\u001b[0m                                 param_types \u001b[38;5;241m=\u001b[39m param_types,\n\u001b[1;32m    109\u001b[0m                                 param_bounds\u001b[38;5;241m=\u001b[39mparam_bounds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m                                 expert_probs\u001b[38;5;241m=\u001b[39msimulated_expert_probs,\n\u001b[1;32m    114\u001b[0m                                 n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'param_names' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "mu_1 = 5 ; sigma_1 = 2 ; a = 2 ; b = 3\n",
    "\n",
    "lambd_names_gaussian_1 = [\"mu_1\", \"sigma_1\", \"a\", \"b\"] ## hyperparameter names\n",
    "lambd_true_vals_gaussian_1 = np.array([mu_1, sigma_1, a, b])\n",
    "param_bounds_gaussian_1 = [[-10., 10.], [0.001, 10.], [0.001, 10.], [0.001, 10.]] ## bounds for each hyperparameter\n",
    "alpha = None\n",
    "\n",
    "target_type = \"continuous\"\n",
    "target_samples = 1500\n",
    "J = 1\n",
    "\n",
    "lower_inner_gaussian_1 = 5 - 20\n",
    "upper_inner_gaussian_1 = 5 + 20\n",
    "\n",
    "\n",
    "\n",
    "simulated_expert_probs_gaussian_1 = np.zeros(len(num_bins))\n",
    "best_params_gaussian_1 = np.zeros(len(num_bins))\n",
    "best_probs_gaussian_1 = np.zeros(len(num_bins))\n",
    "alphas_gaussian_1 = np.zeros(len(num_bins))\n",
    "\n",
    "\n",
    "for i in range(num_bins.shape[0]):\n",
    "\n",
    "    expert_probs, best_params, best_probs, alpha = ppe_simulation(model = gaussian_model_1,\n",
    "                                                                  J = J,\n",
    "                                                                  target_type = target_type,\n",
    "                                                                  lambd_names = lambd_names_gaussian_1,\n",
    "                                                                  lambd_true_vals = lambd_true_vals_gaussian_1,\n",
    "                                                                  alpha = alpha,\n",
    "                                                                  num_bins = num_bins[i],\n",
    "                                                                  lower_inner = lower_inner_gaussian_1,\n",
    "                                                                  upper_inner = upper_inner_gaussian_1,\n",
    "                                                                  param_bounds = param_bounds_gaussian_1,\n",
    "                                                                  target_samples = target_samples)\n",
    "    \n",
    "    simulated_expert_probs_gaussian_1 = expert_probs\n",
    "    best_params_gaussian_1 = best_params\n",
    "    best_probs_gaussian_1 = best_probs\n",
    "    alphas_gaussian_1 = alpha\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "\n",
    "Similar to Model 1, we assume for Model 2 that $Y\\sim \\mathcal{N}(\\mu, \\sigma^2)$, where $\\mu \\sim \\mathcal{N}(\\mu_1, \\sigma_1)$ and $\\sigma \\sim \\text{Gamma}(a, b)$. The difference is that now we also assume hyperpriors for the parameters of $\\mu$'s prior, specifically $\\mu_1\\sim\\mathcal{N}(\\mu_m, \\sigma_m)$ and $\\sigma_1 \\sim \\mathcal{LN}(\\mu_s, \\sigma_s)$, where $\\mathcal{LN}$ is the log-gaussian distribution. This implies a hyperparameter vector $\\pmb{\\lambda} = [\\mu_m, \\sigma_m, \\mu_s, \\sigma_s, a, b]$. For the simulation, we assume that $\\mu_m = 5, \\sigma_m = 1, \\mu_s = 0.4, \\sigma_m = 4, a = 2$ and $b = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppe3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
